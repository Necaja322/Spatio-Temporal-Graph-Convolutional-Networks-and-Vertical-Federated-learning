{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is a model that trains on trafic data more precisely given num_time_step previous speeds it predicts the future forecast_horizion speeds, at the nodes. This is done by combaning temporal convolution over fixed nodes and graph convolution for fixed time intervals, this is combine to a fully connected linear layer to give us our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import typing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.memory.empty_cache() -> None>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.empty_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.L1Loss(reduction='mean')\n",
    "num_epochs = 1000\n",
    "batch_size=16\n",
    "num_time_steps=4*8\n",
    "num_features=1\n",
    "forecast_horizon=1\n",
    "split=3\n",
    "multi_horizon = False\n",
    "patience = 10\n",
    "Chebyshev = True\n",
    "Attention =False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling roads, picking nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 57)\n"
     ]
    }
   ],
   "source": [
    "route_distances = pd.read_csv((r\"C:\\Users\\necad\\OneDrive\\Desktop\\Dataset\\PeMSD7_W_228.csv\"), header=None).to_numpy()\n",
    "speeds_array = pd.read_csv((r\"C:\\Users\\necad\\OneDrive\\Desktop\\Dataset\\PeMSD7_V_228.csv\"), header=None).to_numpy()\n",
    "sample_routes = [\n",
    "    0,1, 4,7,8,11,15,108,109,114,115,118,120,123,124,126,127,129,130,132,133,136,139,144,147,216]\n",
    "sample_routes=[4*i for i in range(57)]\n",
    "route_distances = route_distances[np.ix_(sample_routes, sample_routes)]\n",
    "print(route_distances.shape)\n",
    "speeds_array = speeds_array[:, sample_routes]\n",
    "mean,std= speeds_array.mean(axis=0), speeds_array.std(axis=0)\n",
    "#mean,std= np.round(mean,2), np.round(std,2)\n",
    "#print(f\"Mean is {mean} and std is {std}\")\n",
    "#peeds_array=np.random.random(size=(10000,57,2))\n",
    "#print(speeds_array.shape)\n",
    "num_feature=num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjacency matrix constructor which calculates the Distances to construct a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix(\n",
    "    route_distances: np.ndarray, sigma2: float, epsilon: float\n",
    "):\n",
    "    num_routes = route_distances.shape[0]\n",
    "    route_distances = route_distances / 10000.0\n",
    "    w2, w_mask = (\n",
    "        route_distances * route_distances,\n",
    "        np.ones([num_routes, num_routes]) - np.identity(num_routes),\n",
    "    )\n",
    "    return (np.exp(-w2 / sigma2) >= epsilon) * w_mask+np.eye(num_routes)\n",
    "\n",
    "def adjacency_to_edge_index(adj):\n",
    "    adj = torch.tensor(adj, dtype=torch.float)\n",
    "    \n",
    "    edge_index = adj.nonzero(as_tuple=False).t().contiguous()\n",
    "    \n",
    "    return edge_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 57, number of edges: 139\n"
     ]
    }
   ],
   "source": [
    "class GraphInfo:\n",
    "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
    "        self.edges = edges\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "\n",
    "sigma2 = 0.1\n",
    "epsilon = 0.5\n",
    "adj = compute_adjacency_matrix(route_distances, sigma2, epsilon)\n",
    "node_indices, neighbor_indices = np.where(adj == 1)\n",
    "graph = GraphInfo(\n",
    "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
    "    num_nodes=adj.shape[0],\n",
    ")\n",
    "print(f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\")\n",
    "\n",
    "edge_index=adjacency_to_edge_index(adj)\n",
    "edge_index=edge_index.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precporcess and split data, need to edit for the case of multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: (2956, 57)\n",
      "validation set size: (633, 57)\n",
      "test set size: (634, 57)\n"
     ]
    }
   ],
   "source": [
    "train_size, val_size = 0.7, 0.15\n",
    "def preprocess(data_array: np.ndarray, train_size: float, val_size: float):\n",
    "    num_time_steps = data_array.shape[0]\n",
    "    num_train = int(num_time_steps * train_size)\n",
    "    num_val = int(num_time_steps * val_size)\n",
    "    \n",
    "    train_array = data_array[:num_train]\n",
    "    val_array = data_array[num_train:num_train + num_val]\n",
    "    test_array = data_array[num_train + num_val:]\n",
    "    \n",
    "    return train_array, val_array, test_array\n",
    "\n",
    "# Function to average over rows in the data\n",
    "def average_over_rows(data, split):\n",
    "    # Check the number of dimensions in the input data\n",
    "    if data.ndim == 1:\n",
    "        # If data is 1D, reshape and average\n",
    "        num_elements = data.shape[0]\n",
    "        num_blocks = num_elements // split\n",
    "        averaged_data = np.mean(data[:num_blocks * split].reshape(num_blocks, split), axis=1)\n",
    "        \n",
    "    elif data.ndim == 2:\n",
    "        # If data is 2D, handle separately\n",
    "        num_rows, num_cols = data.shape\n",
    "        num_blocks = num_rows // split\n",
    "        averaged_data = np.mean(data[:num_blocks * split].reshape(num_blocks, split, num_cols), axis=1)\n",
    "        \n",
    "    elif data.ndim == 3:\n",
    "        # If data is 3D, handle separately\n",
    "        num_rows, num_cols, num_depth = data.shape\n",
    "        num_blocks = num_rows // split\n",
    "        averaged_data = np.mean(data[:num_blocks * split].reshape(num_blocks, split, num_cols, num_depth), axis=1)\n",
    "    return averaged_data\n",
    "\n",
    "train, val, test = preprocess(speeds_array, train_size, val_size)\n",
    "\n",
    "train=average_over_rows(train, split)\n",
    "test=average_over_rows(test, split)\n",
    "val=average_over_rows(val, split)\n",
    "\n",
    "print(f\"train set size: {train.shape}\")\n",
    "print(f\"validation set size: {val.shape}\")\n",
    "print(f\"test set size: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series of speeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 2924\n",
      "Test dataset size: 602\n",
      "Validation dataset size: 602\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data_array, num_time_steps, forecast_horizon, multi_horizon=False):\n",
    "        self.data_array = data_array\n",
    "        self.input_sequence_length = num_time_steps\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.multi_horizon = multi_horizon\n",
    "        self.target_offset = (\n",
    "            num_time_steps\n",
    "            if multi_horizon\n",
    "            else num_time_steps + forecast_horizon - 1\n",
    "        )\n",
    "        self.target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "        self.targets = data_array[self.target_offset:]\n",
    "        \n",
    "        # Assuming mean and std are provided or calculated somewhere\n",
    "        mean = np.mean(data_array, axis=(0, 1), keepdims=True)\n",
    "        std = np.std(data_array, axis=(0, 1), keepdims=True)\n",
    "        \n",
    "        # Normalizing the inputs\n",
    "        self.inputs = (data_array[:-forecast_horizon] - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.input_sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx:idx + self.input_sequence_length]\n",
    "        if self.multi_horizon:\n",
    "            target_seq = self.targets[idx: idx + self.target_seq_length]\n",
    "        else:\n",
    "            target_seq = self.targets[idx]\n",
    "        \n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "def create_pytorch_dataset(data_array, input_sequence_length, forecast_horizon, batch_size, multi_horizon=False):\n",
    "    dataset = TimeSeriesDataset(data_array, input_sequence_length, forecast_horizon, multi_horizon)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = (\n",
    "    create_pytorch_dataset(data_array, num_time_steps, forecast_horizon, batch_size=batch_size)\n",
    "    for data_array in [train, val]\n",
    ")\n",
    "\n",
    "test_dataset = create_pytorch_dataset(\n",
    "    test,\n",
    "    num_time_steps,\n",
    "    forecast_horizon,\n",
    "    batch_size=50)\n",
    "\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset.dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(test_dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Neural Network, need to fix architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_horizon:\n",
    "    num_predicted=forecast_horizon\n",
    "else:\n",
    "    num_predicted=1\n",
    " \n",
    "class TemporalGatedConv1d(nn.Module):   # Opperates on (batch_size, in_channels, sequence_length) shape\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        super(TemporalGatedConv1d, self).__init__()\n",
    "        self.conv_gate = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.conv_linear = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate_output = torch.sigmoid(self.conv_gate(x))\n",
    "        linear_output = self.conv_linear(x)\n",
    "        gated_output = gate_output * F.relu(linear_output)\n",
    "        return gated_output\n",
    "\n",
    "class SpatialGraphConv(nn.Module):   \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SpatialGraphConv, self).__init__()\n",
    "        if Chebyshev:\n",
    "            self.conv = ChebConv(in_channels, out_channels, K=2)   \n",
    "        else:\n",
    "            self.conv = GCNConv(in_channels, out_channels)  \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x, edge_index)) \n",
    "\n",
    "class TemporalGraphConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemporalGraphConvNet, self).__init__()\n",
    "        self.temporal_gated_conv1 = TemporalGatedConv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
    "        self.spatial_graph_conv = SpatialGraphConv(in_channels=64, out_channels=16)\n",
    "        \n",
    "        self.spatial_graph_conv2 = SpatialGraphConv(in_channels=1, out_channels=64)\n",
    "        self.temporal_gated_conv2 = TemporalGatedConv1d(in_channels=16, out_channels=64, kernel_size=3)\n",
    "        self.temporal_gated_conv3 = TemporalGatedConv1d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        self.attention1=torch.nn.MultiheadAttention(num_features,1)  #Attention opperates on (seq_length, batch_size, num_features) shape\n",
    "        self.attention2=torch.nn.MultiheadAttention(64,4)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear_layer1 = nn.Linear(64*num_time_steps, 100)\n",
    "        self.linear_layer2 = nn.Linear(100, num_predicted*num_features)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        num_features=num_feature\n",
    "        if num_features == 1:\n",
    "            batch_size, num_time_steps, num_nodes = x.size()\n",
    "        else:\n",
    "            batch_size, num_time_steps, num_nodes, num_features = x.size()\n",
    "\n",
    "\n",
    "        if Attention == True:\n",
    "            x = x.view(num_time_steps, batch_size  * num_nodes,num_features)\n",
    "            x,_ = self.attention1(x,key=x,value=x) \n",
    "            \n",
    "            x = x.view(num_time_steps*batch_size  , num_nodes,1)\n",
    "            x = self.spatial_graph_conv2(x)  # \n",
    "\n",
    "            x = x.view(num_time_steps, batch_size  * num_nodes,64)\n",
    "            x,_= self.attention2(x,key=x,value=x)  \n",
    "        else:\n",
    "            x = x.view(batch_size  * num_nodes,num_features, num_time_steps)\n",
    "            x = self.temporal_gated_conv1(x)  \n",
    "\n",
    "            x = x.view(batch_size*num_time_steps, num_nodes,64)  \n",
    "            x = self.spatial_graph_conv(x)  # \n",
    "\n",
    "            x = x.view(-1, 16, num_time_steps) \n",
    "            x = self.temporal_gated_conv2(x)  \n",
    "\n",
    "            #x = x.view(batch_size*num_time_steps, num_nodes,64)  \n",
    "            #x = self.spatial_graph_conv2(x)  # \n",
    "\n",
    "            #x = x.view(-1, 64, num_time_steps) \n",
    "            #x  = self.temporal_gated_conv3(x)  \n",
    "\n",
    "        # Linear layers\n",
    "        x = x.view(batch_size*num_nodes,64*num_time_steps)\n",
    "        x = F.relu(self.linear_layer1(x))  \n",
    "        \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear_layer2(x)  \n",
    "        \n",
    "        if num_features == 1:\n",
    "            x = x.view(batch_size,num_predicted,num_nodes ) \n",
    "        else:\n",
    "            x = x.view(batch_size,num_predicted,num_nodes,num_features ) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalGraphConvNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "validation_losses=[]\n",
    "train_losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    i=0\n",
    "    for data, targets in train_loader:\n",
    "        i=i+1\n",
    "        data, targets = data.to(device), targets.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        train_losses.append(loss.item()/len(train_loader.dataset))    \n",
    "    return (total_loss )/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            i=i+1\n",
    "            data, targets = data.to(device), targets.to(device)  \n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, targets).item() \n",
    "    return test_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            i=i+1\n",
    "            data, targets = data.to(device), targets.to(device)  \n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, targets).item() \n",
    "    \n",
    "    speed_difference=(test_loss)/i\n",
    "    print(f'Final average speed differnce [{speed_difference:.2f}]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train with early stopping, to find best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, train_loader, val_loader):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader)\n",
    "        val_loss = test(model, val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvment for : {epochs_no_improve} steps\")\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "    model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running, loss is currently set to Mean Squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\necad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([50, 57])) that is different to the input size (torch.Size([50, 1, 57])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\necad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([2, 57])) that is different to the input size (torch.Size([2, 1, 57])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\necad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([16, 57])) that is different to the input size (torch.Size([16, 1, 57])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test loss: 58.42518380972055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\necad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([12, 57])) that is different to the input size (torch.Size([12, 1, 57])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\necad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([9, 57])) that is different to the input size (torch.Size([9, 1, 57])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 16.8216, Val Loss: 8.9971\n",
      "Epoch [2/1000], Train Loss: 9.4549, Val Loss: 8.9075\n",
      "Epoch [3/1000], Train Loss: 9.3102, Val Loss: 8.5442\n",
      "Epoch [4/1000], Train Loss: 9.2504, Val Loss: 8.6146\n",
      "No improvment for : 1 steps\n",
      "Epoch [5/1000], Train Loss: 9.2393, Val Loss: 8.6231\n",
      "No improvment for : 2 steps\n",
      "Epoch [6/1000], Train Loss: 9.1788, Val Loss: 8.3800\n",
      "Epoch [7/1000], Train Loss: 9.2156, Val Loss: 8.4507\n",
      "No improvment for : 1 steps\n",
      "Epoch [8/1000], Train Loss: 9.1766, Val Loss: 8.4011\n",
      "No improvment for : 2 steps\n",
      "Epoch [9/1000], Train Loss: 9.1231, Val Loss: 8.4469\n",
      "No improvment for : 3 steps\n",
      "Epoch [10/1000], Train Loss: 9.1318, Val Loss: 8.3173\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial Test loss: {test(model, test_dataset)}\")\n",
    "train_with_early_stopping(model, train_dataset, val_dataset)\n",
    "test_final(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
