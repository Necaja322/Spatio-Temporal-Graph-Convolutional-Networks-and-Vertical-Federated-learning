{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is a model that trains on trafic data more precisely given num_time_step previous speeds it predicts the future forecast_horizion speeds, at the nodes. This is done by combaning temporal convolution over fixed nodes and graph convolution for fixed time intervals, this is combine to a fully connected linear layer to give us our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import typing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.memory.empty_cache() -> None>"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.empty_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 #step size in gradien descent\n",
    "criterion = nn.L1Loss(reduction='mean')  #loss function\n",
    "num_epochs = 200 #how many times do we go through the whole dataset\n",
    "batch_size=16 #how many samples do we consider at once\n",
    "num_time_steps=12*12#how many intervals back do we consdier\n",
    "forecast_horizon=2 #How far ahead do we forecast forecast_horizon*5min\n",
    "split=2 #consider split*5min intervals\n",
    "multi_horizon = False #forecast multiple horizons meaning predict multiple steps ahead\n",
    "patience = 15 #early stopping patience\n",
    "Chebyshev = True #use Chebyshev convolution instead of GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling roads, picking nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 57)\n"
     ]
    }
   ],
   "source": [
    "route_distances = pd.read_csv((r\"C:\\Users\\necad\\OneDrive\\Desktop\\Dataset\\PeMSD7_W_228.csv\"), header=None).to_numpy()\n",
    "speeds_array = pd.read_csv((r\"C:\\Users\\necad\\OneDrive\\Desktop\\Dataset\\PeMSD7_V_228.csv\"), header=None).to_numpy()  #228\n",
    "sample_routes=[4*i for i in range(57)] #picking nodes to sample\n",
    "route_distances = route_distances[np.ix_(sample_routes, sample_routes)]\n",
    "print(route_distances.shape)\n",
    "speeds_array = speeds_array[:, sample_routes]\n",
    "mean,std= speeds_array.mean(axis=0), speeds_array.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjacency matrix constructor which calculates the Distances to construct a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix(\n",
    "    route_distances: np.ndarray, sigma2: float, epsilon: float\n",
    "):\n",
    "    num_routes = route_distances.shape[0]\n",
    "    route_distances = route_distances / 10000.0\n",
    "    w2, w_mask = (\n",
    "        route_distances * route_distances,\n",
    "        np.ones([num_routes, num_routes]) - np.identity(num_routes),\n",
    "    )\n",
    "    return (np.exp(-w2 / sigma2) >= epsilon) * w_mask+np.eye(num_routes)\n",
    "\n",
    "def adjacency_to_edge_index(adj):\n",
    "    adj = torch.tensor(adj, dtype=torch.float)\n",
    "    \n",
    "    edge_index = adj.nonzero(as_tuple=False).t().contiguous()\n",
    "    \n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 57, number of edges: 139\n"
     ]
    }
   ],
   "source": [
    "class GraphInfo:\n",
    "    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n",
    "        self.edges = edges\n",
    "        self.num_nodes = num_nodes\n",
    "sigma2 = 0.1\n",
    "epsilon = 0.5\n",
    "adj = compute_adjacency_matrix(route_distances, sigma2, epsilon)\n",
    "node_indices, neighbor_indices = np.where(adj == 1)\n",
    "graph = GraphInfo(\n",
    "    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n",
    "    num_nodes=adj.shape[0],\n",
    ")\n",
    "print(f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\")\n",
    "\n",
    "edge_index=adjacency_to_edge_index(adj)\n",
    "edge_index=edge_index.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precporcess and split data, need to edit for the case of multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: (4435, 57)\n",
      "validation set size: (950, 57)\n",
      "test set size: (951, 57)\n"
     ]
    }
   ],
   "source": [
    "train_size, val_size = 0.7, 0.15\n",
    "def preprocess(data_array: np.ndarray, train_size: float, val_size: float):\n",
    "    num_time_steps = data_array.shape[0]\n",
    "    num_train = int(num_time_steps * train_size)\n",
    "    num_val = int(num_time_steps * val_size)\n",
    "    \n",
    "    train_array = data_array[:num_train]\n",
    "    val_array = data_array[num_train:num_train + num_val]\n",
    "    test_array = data_array[num_train + num_val:]\n",
    "    \n",
    "    return train_array, val_array, test_array\n",
    "\n",
    "# Function to average over rows in the data\n",
    "def average_over_rows(data, split):\n",
    "    num_rows, num_cols = data.shape\n",
    "    num_blocks = num_rows // split\n",
    "    averaged_data = np.mean(data[:num_blocks * split].reshape(num_blocks, split, num_cols), axis=1)\n",
    "        \n",
    "    return averaged_data\n",
    "\n",
    "train, val, test = preprocess(speeds_array, train_size, val_size)\n",
    "\n",
    "train=average_over_rows(train, split)\n",
    "test=average_over_rows(test, split)\n",
    "val=average_over_rows(val, split)\n",
    "\n",
    "print(f\"train set size: {train.shape}\")\n",
    "print(f\"validation set size: {val.shape}\")\n",
    "print(f\"test set size: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series of speeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4290\n",
      "Test dataset size: 806\n",
      "Validation dataset size: 806\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data_array, num_time_steps, forecast_horizon, multi_horizon):\n",
    "        self.data_array = data_array\n",
    "        self.input_sequence_length = num_time_steps\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.multi_horizon = multi_horizon\n",
    "        self.target_offset = (\n",
    "            num_time_steps\n",
    "            if multi_horizon\n",
    "            else num_time_steps + forecast_horizon - 1\n",
    "        )\n",
    "        self.target_seq_length = forecast_horizon if multi_horizon == True else 1\n",
    "        self.targets = data_array[self.target_offset:]\n",
    "        \n",
    "        # Assuming mean and std are provided or calculated somewhere\n",
    "        mean = np.mean(data_array, axis=(0, 1), keepdims=True)\n",
    "        std = np.std(data_array, axis=(0, 1), keepdims=True)\n",
    "        \n",
    "        # Normalizing the inputs\n",
    "        self.inputs = (data_array[:-forecast_horizon] - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.input_sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.inputs[idx:idx + self.input_sequence_length]\n",
    "        if self.multi_horizon == True:\n",
    "            target_seq = self.targets[idx: idx + self.target_seq_length]\n",
    "        else:\n",
    "            target_seq = self.targets[idx]\n",
    "        \n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "def create_pytorch_dataset(data_array, input_sequence_length, forecast_horizon, batch_size, multi_horizon=multi_horizon):\n",
    "    dataset = TimeSeriesDataset(data_array, input_sequence_length, forecast_horizon, multi_horizon=multi_horizon)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = (\n",
    "    create_pytorch_dataset(data_array, num_time_steps, forecast_horizon, batch_size=batch_size)\n",
    "    for data_array in [train, val]\n",
    ")\n",
    "\n",
    "test_dataset = create_pytorch_dataset(\n",
    "    test,\n",
    "    num_time_steps,\n",
    "    forecast_horizon,\n",
    "    batch_size=50)\n",
    "\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset.dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(test_dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Neural Network, need to fix architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_horizon:\n",
    "    num_predicted=forecast_horizon\n",
    "else:\n",
    "    num_predicted=1\n",
    "num_features=1 \n",
    "class TemporalGatedConv1d(nn.Module):   # Opperates on (batch_size, in_channels, sequence_length) shape\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        super(TemporalGatedConv1d, self).__init__()\n",
    "        self.conv_gate = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.conv_linear = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate_output = torch.sigmoid(self.conv_gate(x))\n",
    "        linear_output = self.conv_linear(x)\n",
    "        gated_output = gate_output * F.relu(linear_output)\n",
    "        return gated_output\n",
    "\n",
    "class SpatialGraphConv(nn.Module):   \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SpatialGraphConv, self).__init__()\n",
    "        if Chebyshev:\n",
    "            self.conv = ChebConv(in_channels, out_channels, K=2)   \n",
    "        else:\n",
    "            self.conv = GCNConv(in_channels, out_channels)  \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x, edge_index)) \n",
    "\n",
    "class TemporalGraphConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemporalGraphConvNet, self).__init__()\n",
    "        self.temporal_gated_conv1 = TemporalGatedConv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
    "        self.spatial_graph_conv = SpatialGraphConv(in_channels=64, out_channels=64)\n",
    "        self.temporal_gated_conv2 = TemporalGatedConv1d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.spatial_graph_conv2 = SpatialGraphConv(in_channels=64, out_channels=64)\n",
    "        self.temporal_gated_conv3 = TemporalGatedConv1d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear_layer1 = nn.Linear(64*num_time_steps, 256)\n",
    "        if multi_horizon== False:\n",
    "            self.linear_layer2 = nn.Linear(256, 1)\n",
    "        else:\n",
    "            self.linear_layer2 = nn.Linear(256, forecast_horizon)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        num_features=1\n",
    "        batch_size, num_time_steps, num_nodes = x.size()\n",
    "        \n",
    "\n",
    "        x = x.view(batch_size  * num_nodes,num_features, num_time_steps)\n",
    "        x = self.temporal_gated_conv1(x)  \n",
    "\n",
    "        x = x.view(batch_size*num_time_steps, num_nodes,64)  \n",
    "        x = self.spatial_graph_conv(x)  # \n",
    "\n",
    "        x = x.view(-1, 64, num_time_steps) \n",
    "        x = self.temporal_gated_conv2(x)  \n",
    "\n",
    "        x = x.view(batch_size*num_time_steps, num_nodes,64)  \n",
    "        x = self.spatial_graph_conv2(x) \n",
    "\n",
    "\n",
    "        x = x.view(-1, 64, num_time_steps) \n",
    "        x = self.temporal_gated_conv3(x)  \n",
    "        # Linear layers\n",
    "        x = x.view(batch_size*num_nodes,64*num_time_steps)\n",
    "        x = F.relu(self.linear_layer1(x))  \n",
    "        \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear_layer2(x)  \n",
    "        \n",
    "        if multi_horizon== False:\n",
    "            x = x.view(batch_size,num_nodes ) \n",
    "        else:\n",
    "            x = x.view(batch_size,forecast_horizon ,num_nodes) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalGraphConvNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "validation_losses=[]\n",
    "train_losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    relative_errors=[]\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        relative_error=torch.mean(torch.abs( (outputs-targets) /targets))\n",
    "        relative_errors.append(relative_error)    \n",
    "    return (torch.mean(torch.tensor(relative_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader,final=False):\n",
    "    model.eval()\n",
    "    relative_errors=[]\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)  \n",
    "            outputs = model(data)\n",
    "            relative_error=torch.mean(torch.abs( (outputs-targets) /targets))\n",
    "            relative_errors.append(relative_error)  \n",
    "        if final:\n",
    "            print(f'Final test relative error [{torch.mean(torch.tensor(relative_errors)):.2f}]')\n",
    "    return (torch.mean(torch.tensor(relative_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train with early stopping, to find best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, train_loader, val_loader):\n",
    "    best_val_error = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_error = train(model, train_loader)\n",
    "        val_error = test(model, val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train error: {train_error:.2f}, Val error: {val_error:.2f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "\n",
    "        if val_error < best_val_error:\n",
    "            best_val_error = val_error\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvment for : {epochs_no_improve} steps\")\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping')\n",
    "                break            \n",
    "            \n",
    "    model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running, loss is currently set to Mean Squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test relative error: 0.9992169737815857\n",
      "Epoch [1/200], Train error: 0.32, Val error: 0.28\n",
      "Epoch [2/200], Train error: 0.27, Val error: 0.28\n",
      "Epoch [3/200], Train error: 0.27, Val error: 0.29\n",
      "No improvment for : 1 steps\n",
      "Epoch [4/200], Train error: 0.27, Val error: 0.28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial Test relative error: {test(model, test_dataset)}\")\n",
    "train_with_early_stopping(model, train_dataset, val_dataset)\n",
    "test(model, test_dataset,final=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
